name: "local-openhands"
services:
  #---------------------------------------------------------------------------------------------------------
  # LLM推論用コンテナ（llama-server）
  #---------------------------------------------------------------------------------------------------------
  llama-server:
    container_name: local-llama-server
    platform: linux/arm64 
    build:
      context: .
      dockerfile: Dockerfile.llama-server
    gpus: all
    environment:
      # 推論品質や reasoning 制御はコマンドライン引数で指定するので env は最小限
      TZ: Asia/Tokyo
      # MLOCK がサポートされていない環境では 0 に
      USE_MLOCK: "0"
    ports:
      - "18000:8080"  # 外部側はメジャーどころの8000番を回避
    volumes:
      - ${HOME}/models/llm:/models:ro  # ホスト側に保存した GGUF モデルをマウント
    command:
      [
        "--port","8080",
        "-m", "/models/Devstral-Small-2505/Devstral-Small-2505-Q8_0.gguf",
        "-a","devstral-small-2505",
        "-c", "0",
        "--flash-attn", "auto",
        "-ngl", "9999",
        "--jinja",
        "--reasoning-format", "none",
        "-ub", "512",
        "-b", "1024"
      ]
    restart: unless-stopped
  #---------------------------------------------------------------------------------------------------------
  # OpenHands用コンテナ
  #---------------------------------------------------------------------------------------------------------
  openhands:
    container_name: local-openhands
    platform: linux/arm64
    # 実行環境がARMなので、公式（multi-arch）の OpenHands 本体イメージを digest 固定
    image: docker.all-hands.dev/all-hands-ai/openhands:latest@sha256:be67dfe3fb76f2312697bb79cb0b4620a03b266e75be9c953a47d818367df531
    env_file:
      - .env
    environment:
      TZ: Asia/Tokyo
      # LLM_BASE_URL: "http://llama-server:8080/v1"
      # LLM_API_KEY: "dummy-key"
      # LLM_MODEL: "devstral-small-2505"
      LLM_DROP_PARAMS: "true"

      # まずは“勝手に実行しない”運用
      SECURITY_CONFIRMATION_MODE: "true"

      # まずUI確認が目的なので、sandbox起動が必要になったら後述のruntimeへ進む
      # SANDBOX_RUNTIME_CONTAINER_IMAGE: local/openhands-runtime:arm64-v1
      SANDBOX_USER_ID: "${HOST_UID:-1000}"
      SANDBOX_VOLUMES: "${HOME}/workspace:/workspace:rw"
      SANDBOX_RUNTIME_BINDING_ADDRESS: "0.0.0.0"
      SANDBOX_VSCODE_PORT: "46520"
    volumes:
      - ${OPENHANDS_CONFIG_DIR:-${HOME}/.openhands}:/.openhands
      - /usr/bin/docker:/usr/bin/docker:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/libexec/docker/cli-plugins:/usr/libexec/docker/cli-plugins:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      llama-server:
        condition: service_healthy
    ports:
      - "3000:3000"
    restart: unless-stopped

networks:
  default:
    driver: bridge
    name: local-openhands-network
